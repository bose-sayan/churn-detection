{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "# Load the test data\n",
    "test = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a classifier which predicts churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit a model on train data\n",
    "def fit_model(train_data, model):\n",
    "    # First split the data into features and target\n",
    "    X = train_data.drop(columns=[\"Churn\"])\n",
    "    y = train_data[\"Churn\"]\n",
    "\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for the best hyperparameters\n",
    "def search_hyperparameters(train_data, model, param_grid):\n",
    "    # First split the data into features and target\n",
    "    X = train_data.drop(columns=[\"Churn\"])\n",
    "    y = train_data[\"Churn\"]\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"recall\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to score a model on given data\n",
    "def score_model(data, model):\n",
    "    X = data.drop(columns=[\"Churn\"])\n",
    "    y = data[\"Churn\"]\n",
    "    predictions = model.predict(X)\n",
    "    precision = precision_score(y, predictions)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    recall = recall_score(y, predictions)\n",
    "    f1 = f1_score(y, predictions)\n",
    "    return f\"Precision: {precision} Accuracy: {accuracy}, Recall: {recall}, F1: {f1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model predictions\n",
    "def evaluate_model(data, model):\n",
    "    X = data.drop(columns=[\"Churn\"])\n",
    "    y = data[\"Churn\"]\n",
    "    predictions = model.predict(X)\n",
    "    report = classification_report(y, predictions, digits=3)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the adaboost model\n",
    "ada_boost_model = fit_model(train, AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy on train data: Precision: 0.6528384279475983 Accuracy: 0.65, Recall: 0.7119047619047619, F1: 0.6810933940774487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.646     0.582     0.612       380\n",
      "        True      0.653     0.712     0.681       420\n",
      "\n",
      "    accuracy                          0.650       800\n",
      "   macro avg      0.650     0.647     0.647       800\n",
      "weighted avg      0.650     0.650     0.648       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on train data\n",
    "ada_boost_accuracy = score_model(train, ada_boost_model)\n",
    "print(f\"Adaboost accuracy on train data: {ada_boost_accuracy}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(train, ada_boost_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy on test data: Precision: 0.5547445255474452 Accuracy: 0.545, Recall: 0.7169811320754716, F1: 0.6255144032921811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.524     0.351     0.420        94\n",
      "        True      0.555     0.717     0.626       106\n",
      "\n",
      "    accuracy                          0.545       200\n",
      "   macro avg      0.539     0.534     0.523       200\n",
      "weighted avg      0.540     0.545     0.529       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_boost_accuracy = score_model(test, ada_boost_model)\n",
    "print(f\"Adaboost accuracy on test data: {ada_boost_accuracy}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(test, ada_boost_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost confusion matrix on test data:\n",
      "[[33 61]\n",
      " [30 76]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Adaboost confusion matrix on test data:\")\n",
    "print(\n",
    "    confusion_matrix(\n",
    "        test[\"Churn\"], ada_boost_model.predict(test.drop(columns=[\"Churn\"]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Adaboost: {'estimator': DecisionTreeClassifier(max_depth=2), 'learning_rate': 2.2, 'n_estimators': 175}\n"
     ]
    }
   ],
   "source": [
    "# Grid search for best parameters\n",
    "param_grid = {\n",
    "    \"estimator\": [DecisionTreeClassifier(max_depth=2)],\n",
    "    \"n_estimators\": [150, 175, 200],  # Number of boosting rounds\n",
    "    \"learning_rate\": [0.75, 1.0, 1.5, 1.75, 2.0, 2.2, 2.5],  # Learning rate\n",
    "}\n",
    "ada_boost_grid_search = search_hyperparameters(\n",
    "    train, AdaBoostClassifier(algorithm=\"SAMME\", random_state=42), param_grid\n",
    ")\n",
    "print(f\"Best parameters for Adaboost: {ada_boost_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the adaboost model with best parameters\n",
    "ada_boost_model_best = fit_model(\n",
    "    train, AdaBoostClassifier(**ada_boost_grid_search.best_params_, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy on train data: Precision: 0.5514809590973202 Accuracy: 0.56625, Recall: 0.930952380952381, F1: 0.6926483613817538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.681     0.163     0.263       380\n",
      "        True      0.551     0.931     0.693       420\n",
      "\n",
      "    accuracy                          0.566       800\n",
      "   macro avg      0.616     0.547     0.478       800\n",
      "weighted avg      0.613     0.566     0.489       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on train data\n",
    "ada_boost_best = score_model(train, ada_boost_model_best)\n",
    "print(f\"Adaboost accuracy on train data: {ada_boost_best}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(train, ada_boost_model_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy on test data: Precision: 0.5444444444444444 Accuracy: 0.55, Recall: 0.9245283018867925, F1: 0.6853146853146853\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.600     0.128     0.211        94\n",
      "        True      0.544     0.925     0.685       106\n",
      "\n",
      "    accuracy                          0.550       200\n",
      "   macro avg      0.572     0.526     0.448       200\n",
      "weighted avg      0.571     0.550     0.462       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "ada_boost_best = score_model(test, ada_boost_model_best)\n",
    "print(f\"Adaboost accuracy on test data: {ada_boost_best}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(test, ada_boost_model_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 82],\n",
       "       [ 8, 98]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(\n",
    "    test[\"Churn\"], ada_boost_model_best.predict(test.drop(columns=[\"Churn\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random forest model\n",
    "random_forest_model = fit_model(\n",
    "    train, RandomForestClassifier(max_depth=1, random_state=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy on train data: Precision: 0.5293367346938775 Accuracy: 0.5325, Recall: 0.9880952380952381, F1: 0.6893687707641196\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.688     0.029     0.056       380\n",
      "        True      0.529     0.988     0.689       420\n",
      "\n",
      "    accuracy                          0.532       800\n",
      "   macro avg      0.608     0.509     0.372       800\n",
      "weighted avg      0.604     0.532     0.388       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on train data\n",
    "random_forest_accuracy = score_model(train, random_forest_model)\n",
    "print(f\"Random forest accuracy on train data: {random_forest_accuracy}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(train, random_forest_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy on test data: Precision: 0.5303030303030303 Accuracy: 0.53, Recall: 0.9905660377358491, F1: 0.6907894736842105\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.500     0.011     0.021        94\n",
      "        True      0.530     0.991     0.691       106\n",
      "\n",
      "    accuracy                          0.530       200\n",
      "   macro avg      0.515     0.501     0.356       200\n",
      "weighted avg      0.516     0.530     0.376       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "random_forest_accuracy = score_model(test, random_forest_model)\n",
    "print(f\"Random forest accuracy on test data: {random_forest_accuracy}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(test, random_forest_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 1, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# Grid search for best parameters\n",
    "param_grid = {\n",
    "    \"n_estimators\": [175, 200, 225, 250],  # Number of trees in the forest\n",
    "    \"max_depth\": [1, 2, 3, 4],  # Maximum depth of the tree\n",
    "}\n",
    "random_forest_grid_search = search_hyperparameters(\n",
    "    train, RandomForestClassifier(random_state=42), param_grid\n",
    ")\n",
    "print(f\"Best parameters for Random Forest: {random_forest_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_best_model = fit_model(\n",
    "    train,\n",
    "    RandomForestClassifier(**random_forest_grid_search.best_params_, random_state=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy on train data: Precision: 0.5318877551020408 Accuracy: 0.5375, Recall: 0.9928571428571429, F1: 0.6926910299003323\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.812     0.034     0.066       380\n",
      "        True      0.532     0.993     0.693       420\n",
      "\n",
      "    accuracy                          0.537       800\n",
      "   macro avg      0.672     0.514     0.379       800\n",
      "weighted avg      0.665     0.537     0.395       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on train data\n",
    "random_forest_accuracy_best = score_model(train, random_forest_best_model)\n",
    "print(f\"Random forest accuracy on train data: {random_forest_accuracy_best}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(train, random_forest_best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy on test data: Precision: 0.5282051282051282 Accuracy: 0.525, Recall: 0.9716981132075472, F1: 0.6843853820598007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.400     0.021     0.040        94\n",
      "        True      0.528     0.972     0.684       106\n",
      "\n",
      "    accuracy                          0.525       200\n",
      "   macro avg      0.464     0.496     0.362       200\n",
      "weighted avg      0.468     0.525     0.382       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "random_forest_accuracy_best = score_model(test, random_forest_best_model)\n",
    "print(f\"Random forest accuracy on test data: {random_forest_accuracy_best}\")\n",
    "# Evaluate the model predictions\n",
    "print(evaluate_model(test, random_forest_best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,  92],\n",
       "       [  3, 103]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(\n",
    "    test[\"Churn\"], random_forest_best_model.predict(test.drop(columns=[\"Churn\"]))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
